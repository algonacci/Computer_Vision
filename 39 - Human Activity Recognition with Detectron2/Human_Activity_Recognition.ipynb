{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_Activity_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "3FF823_WJNBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJNRjJenI9WG",
        "outputId": "d2627823-0f85-4211-8636-171cac342a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 13 15:42:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehN9FSW2JSKU",
        "outputId": "98962e6b-ebbe-4e4e-b20b-940b608f1c37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and prepare libraries"
      ],
      "metadata": {
        "id": "EsqXvkuPJXW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "5aFNJxSrJcuj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spmallick/learnopencv.git\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "HPXTVR-ZJbXk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd learnopencv/Human-Action-Recognition-Using-Detectron2-And-Lstm\n",
        "!pip install -r requirements.txt\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "03uLHxMeJhQj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert torch.__version__.startswith(\"1.8\")   # need to manually install torch 1.8 if Colab changes its default version\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "0HK-F_hcKLSz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the application"
      ],
      "metadata": {
        "id": "0XMdR5MWKlOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics==0.6.0\n",
        "!pip install --upgrade numpy\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "kJv6RYz7MYit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5TIb4MhOfUN",
        "outputId": "9cd8e188-f882-45e8-ec26-3b14acf9c071"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_final_a6e10b.pkl: 237MB [00:03, 74.1MB/s]               \n",
            "Detectron model loaded in  7.4725377559661865\n",
            " * Serving Flask app \"app\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n",
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            " * Restarting with stat\n",
            "Detectron model loaded in  3.812012195587158\n",
            " * Debugger is active!\n",
            " * Debugger PIN: 831-250-928\n",
            "127.0.0.1 - - [13/Jul/2022 15:49:28] \"\u001b[33mGET /robots.txt HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:49:30] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:49:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:49:37] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:24] \"\u001b[33mGET /robots.txt HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:26] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:28] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:29] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:30] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:30] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:32] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:32] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:34] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:36] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:50:47] \"\u001b[1m\u001b[31mGET /upload HTTP/1.1\u001b[0m\" 405 -\n",
            "files ImmutableMultiDict([('video', <FileStorage: '211212_04_Jakarta_4k_027.mp4' ('video/mp4')>)])\n",
            "filename 211212_04_Jakarta_4k_027.mp4\n",
            "filepath ./211212_04_Jakarta_4k_027.mp4\n",
            "127.0.0.1 - - [13/Jul/2022 15:51:37] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:51:38] \"\u001b[37mGET /analyze/211212_04_Jakarta_4k_027.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:51:40] \"\u001b[37mGET /files/211212_04_Jakarta_4k_027.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "Video processing finished in  80.51101398468018\n",
            "fps  30\n",
            "width height 3840 2160\n",
            "tot_frames 296\n",
            "127.0.0.1 - - [13/Jul/2022 15:53:07] \"\u001b[37mGET /result_video/211212_04_Jakarta_4k_027.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:53:20] \"\u001b[37mGET /analyzed_files/211212_04_Jakarta_4k_027.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:53:25] \"\u001b[37mGET /files/211212_04_Jakarta_4k_027.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:56:18] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:56:33] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:56:33] \"\u001b[37mGET /analyze/sample_video.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [13/Jul/2022 15:56:51] \"\u001b[37mGET /files/sample_video.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
            "Video processing finished in  36.321656942367554\n",
            "fps  30\n",
            "width height 1280 720\n",
            "tot_frames 304\n",
            "127.0.0.1 - - [13/Jul/2022 15:57:10] \"\u001b[37mGET /result_video/sample_video.mp4 HTTP/1.1\u001b[0m\" 200 -\n",
            "finished video streaming\n"
          ]
        }
      ]
    }
  ]
}